<!doctype html><html lang=en dir=ltr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Preliminaries #1: Diffusion Models | Generative Models | GONG Linxia</title><meta name=generator content="Hugo Eureka 0.9.3"><link rel=stylesheet href=https://linxiagong.github.io/css/eureka.min.9590cc4ee18e71bf79bee9cd054d9b06e74d5095459d717098908afdc5b5bf05c10426f0a0f1fbc02b5f09bb2c6f5c5a.css><script defer src=https://linxiagong.github.io/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto+Serif+SC:wght@400;600;700&amp;display=swap" as=style onload='this.onload=null,this.rel="stylesheet"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js crossorigin></script>
<link rel=stylesheet href=https://linxiagong.github.io/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css media=print onload='this.media="all",this.onload=null'><script defer type=text/javascript src=https://linxiagong.github.io/js/fontawesome.min.1ff14f2c249de0496817e0d57205f59911b9140b927fd0d01dfdf35dc40c08318b9ca5c84b121c6dadbbeb90676a5bb8.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js integrity=sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0 crossorigin></script>
<link rel=icon type=image/png sizes=32x32 href=https://linxiagong.github.io/images/icon_hu6af084b4776a16952bc10dec2601ce33_2197728_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=https://linxiagong.github.io/images/icon_hu6af084b4776a16952bc10dec2601ce33_2197728_180x180_fill_box_center_3.png><meta name=description content="Eureka is a elegant and powerful theme for Hugo."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://linxiagong.github.io/docs/"},{"@type":"ListItem","position":2,"name":"Generative Models","item":"https://linxiagong.github.io/docs/generative-models/"},{"@type":"ListItem","position":3,"name":"Preliminaries #1: Diffusion Models","item":"https://linxiagong.github.io/docs/generative-models/diffusion/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://linxiagong.github.io/docs/generative-models/diffusion/"},"headline":"Preliminaries #1: Diffusion Models | Generative Models | GONG Linxia","wordCount":1017,"publisher":{"@type":"Person","name":"WANG Chucheng","logo":{"@type":"ImageObject","url":"https://linxiagong.github.io/images/icon.png"}},"description":"Eureka is a elegant and powerful theme for Hugo."}</script><meta property="og:title" content="Preliminaries #1: Diffusion Models | Generative Models | GONG Linxia"><meta property="og:type" content="website"><meta property="og:image" content="https://linxiagong.github.io/images/icon.png"><meta property="og:url" content="https://linxiagong.github.io/docs/generative-models/diffusion/"><meta property="og:description" content="Eureka is a elegant and powerful theme for Hugo."><meta property="og:locale" content="en"><meta property="og:site_name" content="GONG Linxia"><meta property="og:updated_time" content="2023-06-05T00:00:00+00:00"><meta property="article:section" content="docs"><link rel=alternate type=application/rss+xml href=https://linxiagong.github.io/docs/generative-models/diffusion/index.xml title="GONG Linxia"><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");((storageColorScheme=="Auto"||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName("html")[0].classList.add("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="me-6 text-primary-text text-xl font-bold">GONG Linxia</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/#about class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">About</a>
<a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Posts</a>
<a href=/docs/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item me-4">Projects</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>Light</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>Dark</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>Auto</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById("lightDarkMode");storageColorScheme==null||storageColorScheme=="Auto"?document.addEventListener("DOMContentLoaded",()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",switchDarkMode)}):storageColorScheme=="Light"?(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","sun"),element.firstElementChild.classList.add("fa-sun")):storageColorScheme=="Dark"&&(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","moon"),element.firstElementChild.classList.add("fa-moon")),document.addEventListener("DOMContentLoaded",()=>{getcolorscheme(),switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8"><div class=lg:pt-12><div class="flex flex-col md:flex-row bg-secondary-bg rounded"><div class="md:w-1/4 lg:w-1/5 border-e"><div class="sticky top-16 pt-6"><div id=sidebar-title class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text"><span class=font-semibold>Table of Contents</span>
<i class='fas fa-caret-right ms-1'></i></div><div id=sidebar-toc class="hidden md:block overflow-y-auto mx-6 md:mx-0 pe-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent"><div class="flex flex-wrap ms-4 -me-2 p-2 bg-secondary-bg md:bg-primary-bg rounded"><a class=hover:text-eureka href=https://linxiagong.github.io/docs/generative-models/>Generative Models</a></div><ul class=ps-6><li class=py-2><div class=pb-2><a class="text-eureka hover:text-eureka" href=https://linxiagong.github.io/docs/generative-models/diffusion/>Preliminaries #1: Diffusion Models</a></div><ul class=ps-6></ul></li><li class=py-2><div class=pb-2><a class=hover:text-eureka href=https://linxiagong.github.io/docs/generative-models/gen-3d/>Generative 3D Models</a></div><ul class=ps-6></ul></li><li class=py-2><div class=pb-2><a class=hover:text-eureka href=https://linxiagong.github.io/docs/generative-models/lora/>Preliminaries: Low-Rank Adaption (LoRA)</a></div><ul class=ps-6></ul></li></ul></div></div></div><div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8"><div class=flex><div class="w-full lg:w-3/4 px-6"><article class=prose><h1 class=mb-4>Preliminaries #1: Diffusion Models</h1><div class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"><div class="me-6 my-2"><i class="fas fa-calendar me-1"></i>
<span>2023-06-05</span></div><div class="me-6 my-2"><i class="fas fa-clock me-1"></i>
<span>5 min read</span></div></div><h2 id=overview>Overview</h2><blockquote style=font-style:normal;font-size:14px>This blog aims to provide an intuitive introduction to Diffusion Models. For further in-depth reading, several highly recommended blogs are available:<ul><li>For beginners:<ul><li><a href=https://calvinyluo.com/2022/08/26/diffusion-tutorial.html>Understanding Diffusion Models: A Unified Perspective</a> by Calvin Luo</li></ul></li><li>For more details:<ul><li><a href=https://lilianweng.github.io/posts/2021-07-11-diffusion-models/>What are Diffusion Models?</a> by Lilian Weng</li><li><a href=https://yang-song.net/blog/2021/score/>Generative Modeling by Estimating Gradients of the Data Distribution</a> by Yang Song</li></ul></li></ul></blockquote><p>Given observed samples $x$ from a distribution of interest, <strong>the goal of a generative model is to learn to model its true data distribution $p(x)$</strong>. Once learned, we can generate new samples from our approximate model at will.
Furthermore, under some formulations, we are able to use the learned model to evaluate the likelihood of observed or sampled data as well.</p><p><img src=models.jpg alt="Generative Models"></p><h2 id=background>Background</h2><h3 id=vae--elbo>VAE & ELBO</h3><p>Before introducing Diffusion Model, let&rsquo;s just revise the <em><b>V</b>ariational <b>A</b>uto<b>E</b>ncoder (VAE)</em> and <em><b>E</b>vidence <b>L</b>ower <b>Bo</b>und (ELBO)</em>.</p><p style=margin-bottom:0><a href=https://arxiv.org/abs/1312.6114 style=text-decoration-color:#68a0b4;color:#68a0b4 title="Auto-Encoding Variational Bayes"><b>Variational Autoencoder</b></a> introduced a latent variable $z$ to learn the underlying latent structure that describes our observed data $x$.</p><div style=text-align:center;margin-bottom:0><figure style=display:inline-block;margin-bottom:0;padding:0><center><img style=margin:0;width:200px src=vae.png alt=Image></center><figcaption style=text-align:center;margin:0>Vanilla Variational Autoencoder.<br>A latent encoder and decoder are learned jointly through the reparameterization trick.</figcaption></figure></div><p>Mathematically, we can imagine the latent variables and the data we observe as modeled by a joint distribution $p(x, z)$.
Likelihood-based generative modeling, like VAE, <strong>learns a model to maximize the likelihood $p(x)$ of all observed $x$.</strong>
There are two ways we can manipulate this joint distribution to recover the likelihood of purely our observed data $p(x)$
; we can explicitly <a href=https://en.wikipedia.org/wiki/Marginal_likelihood>marginalize</a> out the latent variable $z$:</p><div style=margin:0 id=eq1>\[\begin{aligned} &p(x)= \int p(x, z)dz &(1) \end{aligned}\]</div><span style=color:#a9a9a9><b>[Computing and maximizing the likelihood $p(x)$ is difficult]</b> it involves integrating out all latent variables $z$. This is intractable for complex models.</span><br>or, we could also appeal to the <a href=https://en.wikipedia.org/wiki/Chain_rule_(probability)>chain rule of probability</a>:<div id=eq2>\[\begin{aligned} &p(x)=\frac{p(x, z)}{p(z|x)} &(2) \end{aligned}\]</div><span style=color:#a9a9a9><b>[Computing and maximizing the likelihood $p(x)$ is difficult]</b> it involves having access to a ground truth latent encoder $p(z|x)$.</span><br>Using these two equations, we can derive a term called the Evidence Lower Bound (ELBO).<p><b style=color:#68a0b4>Evidence Lower Bound (ELBO)</b>, which as its name suggests, is a lower bound of the evidence. In this case, the <strong>evidence</strong> is quantified as the log likelihood of the observed data, denoted as $\log p(x)$.</p><p>Let&rsquo;s derive ELBO using the chain rule marginalization method, as in <a href=#eq2>Equation 2</a>:</p><div>\[\begin{aligned}
\log p(x) &= \log p(x) \int q_\phi(z|x)dz  & \small{(\int q_\phi(z|x)dz=1)}\\
&= \int q_\phi(z|x) \log p(x)dz  \\
&= \mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log p(x) \boldsymbol] \\
&= \mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log \frac{p(x,z)}{p(z|x)} \boldsymbol] \\
&= \mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log \frac{p(x,z)q_\phi(z|x)}{p(z|x)q_\phi(z|x)} \boldsymbol] \\
&= \mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log \frac{p(x,z)}{q_\phi(z|x)} \boldsymbol] + \mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log \frac{q_\phi(z|x)}{p(z|x)} \boldsymbol]\\
&= \mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log \frac{p(x,z)}{q_\phi(z|x)} \boldsymbol] + \text{D}_{\text{KL}}\boldsymbol( q_\phi(z|x) \,||\, p(z|x) \boldsymbol) &\text{\small(KL definition)} \\
 &\ge \underbrace{\mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log \frac{p(x,z)}{q_\phi(z|x)} \boldsymbol]}_{\text{ELBO}} &\small{(\text{D}_{\text{KL}}\text{ is non-negative)}}
\end{aligned}\]</div><details style="border:1px solid #ccc;padding:10px;border-radius:5px"><summary>Derive ELBO using the integration marginalization and Jensen's inequality (optional)</summary>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There's an alternative method to derive ELBO from evidence, commonly found in blogs introducing Diffusion Models. <b>However, I don't personally recommend this approach as it fails to provide sufficient intuition as to why we opt for ELBO to optimize generative models.</b><p>Jensen&rsquo;s inequality states that for any concave function $f(x)$, if $X$ is a random variable, then:</p><div>\[ f(\mathbb{E}[X]) \geq \mathbb{E}[f(X)] \]</div><p>Let&rsquo;s start from <a href=#eq1>Equation 1</a>, we can derive ELBO:</p><div>\[\begin{aligned}
\log p(x) &= \log \int p(x, z)dz   &\small{(p(x)=\int p(x, z)dz)} \\
&= \log \int \frac{p(x, z)q_\phi(z|x)}{q_\phi(z|x)} dz \\
&= \log \mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \frac{p(x, z)}{q_\phi(z|x)} \boldsymbol] \\
&\ge \underbrace{\mathbb{E}_{q_\phi(z|x)}\boldsymbol[ \log \frac{p(x,z)}{q_\phi(z|x)} \boldsymbol]}_{\text{ELBO}} &\text{\small{(Jensen's inequality)}}
\end{aligned}\]</div><p>      In this derivation, we directly arrive at our lower bound by applying Jensen’s Inequality. However, this does not supply us much useful information about what is actually going on underneath the hood; crucially, this proof gives no intuition on exactly why the ELBO is actually a lower bound of the evidence, as Jensen’s Inequality handwaves it away. Furthermore, simply knowing that the ELBO is truly a lower bound of the data does not really tell us why we want to maximize it as an objective.</p></details><i>Notice that the likelihood of our data, i.e. the evidence term $\log p(x)$, is always a constant with respect to $\phi$, as it is computed by marginalizing out all latents $z$ from the joint distribution $p(x,z)$ and does not depend on $\phi$ whatsoever.</i>
Since ELBO and KL Divergence sum up to a constant, <b>maximizing ELBO is equivalent to minimizing KL Divergence between our approximate posterior distribution and true posterior distribution</b>. Thus we can easily utilize ELBO as a proxy objective to optimize a latent variable model, that is, by powerfully parameterizing and perfectly optimizing the ELBO, we can make our approximate posterior $q_\phi(z|x)$ exactly equivalent to the true latent posterior $p(z|x)$.<p>We can rewrite ELBO into a form of VAE objective:</p><div>\[\begin{aligned}
\underbrace{\mathbb{E}_{q_{\phi}(z|x)}\boldsymbol[\log \frac{p(x,z)}{q_\phi(z|x)}\boldsymbol]}_{\text{ELBO}} &= \mathbb{E}_{q_{\phi}(z|x)}\boldsymbol[\log \frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}\boldsymbol]\\
&= \mathbb{E}_{q_{\phi}(z|x)}\boldsymbol[\log p_\theta(x|z)\boldsymbol] + \mathbb{E}_{q_{\phi}(z|x)}\boldsymbol[\log \frac{p(z)}{q_\phi(z|x)}\boldsymbol] \\
&= \underbrace{\mathbb{E}_{q_{\phi}(z|x)}\boldsymbol[\log p_\theta(x|z)\boldsymbol]}_{\text{reconstruction term}} - \underbrace{D_{\text{KL}}\boldsymbol(q_\phi(z|x) \,||\, p(z)\boldsymbol)}_{\text{prior matching term}}
\end{aligned}\]</div>After reformulation, we obtain the training objective of VAE. Optimizing over this objective is equivalent to approximating the true data distribution, and at the same time, optimizing the posterior latent vector distribution $q(z|x)$ using variational methods.<br><span style=font-size:14px;color:#a9a9a9><b>Variational</b> is the term used for variational inference, a technique that approximates complex probability distributions using simpler ones. It finds the distribution in a specific class that is closest to the true posterior distribution. In VAE, it refers to the fact that we optimize for the best $q_\phi(z|x)$ amongst a family of potential posterior distributions parameterized by $\phi$.</span><p>After training a VAE, generating new data can be performed by sampling directly from the latent space and then running it through the decoder.</p><h3 id=hvae>HVAE</h3><p><b style=color:#68a0b4>Hierarchical Variational Autoencoder (HVAE)</b> is a generalization of a VAE that extends to multiple hierarchies over latent variables. Under this formulation, latent variables themselves are interpreted as generated from other higher-level, more abstract latents.</p><div style=text-align:center;margin-bottom:0><figure style=display:inline-block;width:500px;margin:0;padding:0><img style=margin:0 src=MHVAE.jpg alt=Image><figcaption style=text-align:center;margin:0>Hierarchical Variational Autoencoder (HVAE)</figcaption></figure></div>We focus on a special case <b style=color:#68a0b4>Markovian HVAE (MHVAE)</b>, in which each transition down the hierarchy is Markovian, where decoding each latent $z_t$ only conditions on previous latent $z_{t+1}$.
Intuitively, and visually, this can be seen as simply stacking VAEs on top of each other, in a form of Recursive VAE.
Mathematically, we can get the joint distribution of a Markovian HVAE and its posterior as<div>\[ p(x, z_{1:T}) = p(z_T)p_{\theta}(x|z_1)\prod_{t=2}^{T} p_{\theta}(z_{t-1}|z_t) \]
\[ q_{\phi}(z_{1:T}|x) = q_{\phi}(z_1|x)\prod_{t=2}^{T} q_{\phi}(z_t|z_{t-1}) \]</div><mark>Variational Diffusion Models: TBD</mark></article></div><div class="hidden lg:block lg:w-1/4"><div class="bg-secondary-bg
prose sticky top-16 z-10 hidden px-6 py-4 lg:block"><h3>On This Page</h3></div><div class="sticky-toc
border-s
hidden px-6 pb-6 lg:block"><nav id=TableOfContents><ul><li><a href=#overview>Overview</a></li><li><a href=#background>Background</a><ul><li><a href=#vae--elbo>VAE & ELBO</a></li><li><a href=#hvae>HVAE</a></li></ul></li></ul></nav></div><script>window.addEventListener("DOMContentLoaded",()=>{enableStickyToc()})</script></div></div></div></div></div><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll(),changeSidebarHeight(),switchDocToc()})</script></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2021 <a href=https://www.wangchucheng.com/>WANG Chucheng</a> and <a href=https://www.ruiqima.com/>MA Ruiqi</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>