<!doctype html><html lang=en dir=ltr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Preliminaries: Low-Rank Adaption (LoRA) | Generative Models | GONG Linxia</title><meta name=generator content="Hugo Eureka 0.9.3"><link rel=stylesheet href=https://linxiagong.github.io/css/eureka.min.9590cc4ee18e71bf79bee9cd054d9b06e74d5095459d717098908afdc5b5bf05c10426f0a0f1fbc02b5f09bb2c6f5c5a.css><script defer src=https://linxiagong.github.io/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&amp;family=Noto+Serif+SC:wght@400;600;700&amp;display=swap" as=style onload='this.onload=null,this.rel="stylesheet"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js crossorigin></script>
<link rel=stylesheet href=https://linxiagong.github.io/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css media=print onload='this.media="all",this.onload=null'><script defer type=text/javascript src=https://linxiagong.github.io/js/fontawesome.min.1ff14f2c249de0496817e0d57205f59911b9140b927fd0d01dfdf35dc40c08318b9ca5c84b121c6dadbbeb90676a5bb8.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js integrity=sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0 crossorigin></script>
<link rel=icon type=image/png sizes=32x32 href=https://linxiagong.github.io/images/icon_hu6af084b4776a16952bc10dec2601ce33_2197728_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=https://linxiagong.github.io/images/icon_hu6af084b4776a16952bc10dec2601ce33_2197728_180x180_fill_box_center_3.png><meta name=description content="A brief introduction to the fine-tuing method -- LoRA."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://linxiagong.github.io/docs/"},{"@type":"ListItem","position":2,"name":"Generative Models","item":"https://linxiagong.github.io/docs/generative-models/"},{"@type":"ListItem","position":3,"name":"Preliminaries: Low-Rank Adaption (LoRA)","item":"https://linxiagong.github.io/docs/generative-models/lora/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://linxiagong.github.io/docs/generative-models/lora/"},"headline":"Preliminaries: Low-Rank Adaption (LoRA) | Generative Models | GONG Linxia","wordCount":460,"publisher":{"@type":"Person","name":"WANG Chucheng","logo":{"@type":"ImageObject","url":"https://linxiagong.github.io/images/icon.png"}},"description":"A brief introduction to the fine-tuing method -- LoRA."}</script><meta property="og:title" content="Preliminaries: Low-Rank Adaption (LoRA) | Generative Models | GONG Linxia"><meta property="og:type" content="website"><meta property="og:image" content="https://linxiagong.github.io/images/icon.png"><meta property="og:url" content="https://linxiagong.github.io/docs/generative-models/lora/"><meta property="og:description" content="A brief introduction to the fine-tuing method -- LoRA."><meta property="og:locale" content="en"><meta property="og:site_name" content="GONG Linxia"><meta property="og:updated_time" content="2023-06-08T00:00:00+00:00"><meta property="article:section" content="docs"><link rel=alternate type=application/rss+xml href=https://linxiagong.github.io/docs/generative-models/lora/index.xml title="GONG Linxia"><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");((storageColorScheme=="Auto"||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName("html")[0].classList.add("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="me-6 text-primary-text text-xl font-bold">GONG Linxia</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/#about class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">About</a>
<a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Posts</a>
<a href=/docs/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item me-4">Projects</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>Light</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>Dark</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>Auto</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById("lightDarkMode");storageColorScheme==null||storageColorScheme=="Auto"?document.addEventListener("DOMContentLoaded",()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",switchDarkMode)}):storageColorScheme=="Light"?(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","sun"),element.firstElementChild.classList.add("fa-sun")):storageColorScheme=="Dark"&&(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","moon"),element.firstElementChild.classList.add("fa-moon")),document.addEventListener("DOMContentLoaded",()=>{getcolorscheme(),switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8"><div class=lg:pt-12><div class="flex flex-col md:flex-row bg-secondary-bg rounded"><div class="md:w-1/4 lg:w-1/5 border-e"><div class="sticky top-16 pt-6"><div id=sidebar-title class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text"><span class=font-semibold>Table of Contents</span>
<i class='fas fa-caret-right ms-1'></i></div><div id=sidebar-toc class="hidden md:block overflow-y-auto mx-6 md:mx-0 pe-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent"><div class="flex flex-wrap ms-4 -me-2 p-2 bg-secondary-bg md:bg-primary-bg rounded"><a class=hover:text-eureka href=https://linxiagong.github.io/docs/generative-models/>Generative Models</a></div><ul class=ps-6><li class=py-2><div class=pb-2><a class=hover:text-eureka href=https://linxiagong.github.io/docs/generative-models/gen-3d/>Generative 3D Models</a></div><ul class=ps-6></ul></li><li class=py-2><div class=pb-2><a class="text-eureka hover:text-eureka" href=https://linxiagong.github.io/docs/generative-models/lora/>Preliminaries: Low-Rank Adaption (LoRA)</a></div><ul class=ps-6></ul></li></ul></div></div></div><div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8"><div class=flex><div class="w-full lg:w-3/4 px-6"><article class=prose><h1 class=mb-4>Preliminaries: Low-Rank Adaption (LoRA)</h1><div class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"><div class="me-6 my-2"><i class="fas fa-calendar me-1"></i>
<span>2023-06-08</span></div><div class="me-6 my-2"><i class="fas fa-clock me-1"></i>
<span>3 min read</span></div></div><figure style="float:right;margin-left:10px;width:250px;padding:5px;box-shadow:2px 2px 4px rgba(0,0,0,.3);border-radius:10px"><img src=reparameterization.jpg alt=Image><figcaption style=text-align:center>LoRA Reparameterization:<br>we only train A and B.</figcaption></figure><p><a href=https://github.com/microsoft/LoRA>LoRA: Low-Rank Adaptation of Large Language Models</a> is a commonly used technique to deal with the problem of fine-tuning large-language models.</p><p>LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. This vastly reduces the storage requirement for large language models adapted to specific tasks and enables efficient task-switching during deployment all without introducing inference latency.</p><h2 id=motivation>Motivation</h2><p>It is commonplace to utilize foundation models that have undergone pre-training on extensive datasets. These models possess pre-existing knowledge and are typically more adept at mastering unseen, yet similar tasks.
This allows us to leverage the knowledge gained by a pre-trained model and adapt it to a specific task or dataset with relatively less training data.
This process, known as <strong>fine-tuning</strong>, involves taking a pre-trained model and further training it on a specific task or dataset to enhance its performance in that particular domain.</p><p><cite><strong>How does fine-tuning work?</strong><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite></p><p><b style=color:#68a0b4>Classical fine-tuing.</b> During the fine-tuning process, the pre-trained model is presented with input from the new task-specific dataset.
It then generates predictions and compares them with the ground truth. Through iterative adjustments to the model&rsquo;s weights, as illustrated in <a href=#fig2>Fig 2.a</a>, the model refines its predictions. Repeating this process over multiple iterations allows the pre-trained model to become fine-tuned for the downstream task.</p><div style=display:flex;justify-content:space-between id=fig2><figure style=width:32%;margin:0><img style=margin:0 src=Finetuing-classical.jpg alt=Image><figcaption style=text-align:center>Fig 2.a) Classic ML model<br>fine-tuning</figcaption></figure><figure style=width:32%;margin:0><img style=margin:0 src=Finetuing-split-weights.jpg alt=Image><figcaption style=text-align:center>Fig 2.b) Freezing the original ML model weights, fine-tuning a separate set of weights</figcaption></figure><figure style=width:32%;margin:0><img style=margin:0 src=Finetuning-lora.jpg alt=Image><figcaption style=text-align:center>Fig 2.c) Low-rank adaptation (LoRA)</figcaption></figure></div><p><b style=color:#68a0b4>Adapter tuing.</b> Later, we adopt a new technique for the fine-tuning process, depicted in <a href=#fig2>Fig 2.b</a>. In this new method, we freeze the original weights of the model and refrain from modifying them during the fine-tuning process. Instead, we introduce a separate set of weights where we apply the necessary modifications. We refer to these two sets as the &ldquo;pre-trained&rdquo; and &ldquo;fine-tuned&rdquo; weights, respectively.</p><p><b style=color:#68a0b4>LoRA.</b> Furthermore, LoRA, as shown in <a href=#fig2>Fig 2.c</a>, suggests that the full-rank weight matrix is not necessary when fine-tuning a large model for a downstream task. We can actually reduce the dimension of the downstream parameters while preserving most of the learning capacity of the model.
In essence, LoRA uses two downstream weight matrices: one to transform the input parameters from the original dimension to the low-rank dimension, and another to transform the low-rank data to match the output dimensions of the original model.
During training, modifications are made to the LoRA parameters, which are significantly fewer than the original weights (hence allowing faster training and reduces the cost compared to full fine-tuning).
At inference time, the output of LoRA is added to the pre-trained parameters to calculate the final values.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://bdtechtalks.com/2023/05/22/what-is-lora/>What is low-rank adaptation (LoRA)?</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></article></div><div class="hidden lg:block lg:w-1/4"><div class="bg-secondary-bg
prose sticky top-16 z-10 hidden px-6 py-4 lg:block"><h3>On This Page</h3></div><div class="sticky-toc
border-s
hidden px-6 pb-6 lg:block"><nav id=TableOfContents><ul><li><a href=#motivation>Motivation</a></li></ul></nav></div><script>window.addEventListener("DOMContentLoaded",()=>{enableStickyToc()})</script></div></div></div></div></div><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll(),changeSidebarHeight(),switchDocToc()})</script></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2021 <a href=https://www.wangchucheng.com/>WANG Chucheng</a> and <a href=https://www.ruiqima.com/>MA Ruiqi</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>