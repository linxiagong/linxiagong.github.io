<!doctype html><html lang=en dir=ltr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Reinforcement Learning for Operation Research I | GONG Linxia</title><meta name=generator content="Hugo Eureka 0.9.3"><link rel=stylesheet href=https://linxiagong.github.io/css/eureka.min.bdf58ace07b035e182df72eefd38164d659d22fe075601facf4adae076677a1f2f40b61bd9c1989777f0a9138d787dcd.css><script defer src=https://linxiagong.github.io/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap" as=style onload='this.onload=null,this.rel="stylesheet"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js crossorigin></script>
<link rel=stylesheet href=https://linxiagong.github.io/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css media=print onload='this.media="all",this.onload=null'><script defer type=text/javascript src=https://linxiagong.github.io/js/fontawesome.min.a1b3429ed9b7466d05bae91b07c51ccaf376a4ed4d9c8e3f794916f99e957a727dfb793919e2d33506ecca9cc5ba978f.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js integrity=sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0 crossorigin></script>
<link rel=icon type=image/png sizes=32x32 href=https://linxiagong.github.io/images/icon_hu6af084b4776a16952bc10dec2601ce33_2197728_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=https://linxiagong.github.io/images/icon_hu6af084b4776a16952bc10dec2601ce33_2197728_180x180_fill_box_center_3.png><meta name=description content="In early 2022, I made a survey about the frontier of Reinforcement Learning (RL) in Operations Research (OR). As an AI researcher, I was interested in the application of RL in solving complex optimization problems and tried to figure out if there is any possible work that I can contribute. Through my survey, I gained somehow an understanding of the state-of-the-art in this field. Although I eventually shifted my focus to other areas, I hereby would like to share my insights and possibly facilitate future research."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://linxiagong.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Reinforcement Learning for Operation Research I","item":"https://linxiagong.github.io/posts/rl4or/rl4or-1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://linxiagong.github.io/posts/rl4or/rl4or-1/"},"headline":"Reinforcement Learning for Operation Research I | GONG Linxia","datePublished":"2023-02-07T22:46:18+08:00","dateModified":"2023-02-07T22:46:18+08:00","wordCount":687,"publisher":{"@type":"Person","name":"WANG Chucheng","logo":{"@type":"ImageObject","url":"https://linxiagong.github.io/images/icon.png"}},"description":"In early 2022, I made a survey about the frontier of Reinforcement Learning (RL) in Operations Research (OR). As an AI researcher, I was interested in the application of RL in solving complex optimization problems and tried to figure out if there is any possible work that I can contribute. Through my survey, I gained somehow an understanding of the state-of-the-art in this field. Although I eventually shifted my focus to other areas, I hereby would like to share my insights and possibly facilitate future research."}</script><meta property="og:title" content="Reinforcement Learning for Operation Research I | GONG Linxia"><meta property="og:type" content="article"><meta property="og:image" content="https://linxiagong.github.io/images/icon.png"><meta property="og:url" content="https://linxiagong.github.io/posts/rl4or/rl4or-1/"><meta property="og:description" content="In early 2022, I made a survey about the frontier of Reinforcement Learning (RL) in Operations Research (OR). As an AI researcher, I was interested in the application of RL in solving complex optimization problems and tried to figure out if there is any possible work that I can contribute. Through my survey, I gained somehow an understanding of the state-of-the-art in this field. Although I eventually shifted my focus to other areas, I hereby would like to share my insights and possibly facilitate future research."><meta property="og:locale" content="en"><meta property="og:site_name" content="GONG Linxia"><meta property="article:published_time" content="2023-02-07T22:46:18+08:00"><meta property="article:modified_time" content="2023-02-07T22:46:18+08:00"><meta property="article:section" content="posts"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Operation Research"><meta property="article:tag" content="Combinatorial Optimization"><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");((storageColorScheme=="Auto"||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName("html")[0].classList.add("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="me-6 text-primary-text text-xl font-bold">GONG Linxia</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/#about class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">About</a>
<a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item me-4">Posts</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-adjust"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>Light</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>Dark</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>Auto</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById("lightDarkMode");storageColorScheme==null||storageColorScheme=="Auto"?document.addEventListener("DOMContentLoaded",()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",switchDarkMode)}):storageColorScheme=="Light"?(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","sun"),element.firstElementChild.classList.add("fa-sun")):storageColorScheme=="Dark"&&(element.firstElementChild.classList.remove("fa-adjust"),element.firstElementChild.setAttribute("data-icon","moon"),element.firstElementChild.classList.add("fa-moon")),document.addEventListener("DOMContentLoaded",()=>{getcolorscheme(),switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8"><div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12"><div class="bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"><article class=prose><h1 class=mb-4>Reinforcement Learning for Operation Research I</h1><div class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"><div class="me-6 my-2"><i class="fas fa-calendar me-1"></i>
<span>2023-02-07</span></div><div class="me-6 my-2"><i class="fas fa-clock me-1"></i>
<span>4 min read</span></div><div class="me-6 my-2"><i class="fas fa-folder me-1"></i>
<a href=https://linxiagong.github.io/categories/rl/ class=hover:text-eureka>RL</a>
<span>,</span>
<a href=https://linxiagong.github.io/categories/or/ class=hover:text-eureka>OR</a></div><div class="me-6 my-2"><i class="fas fa-th-list me-1"></i>
<a href=https://linxiagong.github.io/series/reinforcement-learning-for-operation-research/ class=hover:text-eureka>Reinforcement Learning for Operation Research</a></div></div><p>In early 2022, I made a survey about the frontier of Reinforcement Learning (RL) in Operations Research (OR). As an AI researcher, I was interested in the application of RL in solving complex optimization problems and tried to figure out if there is any possible work that I can contribute. Through my survey, I gained somehow an understanding of the state-of-the-art in this field. Although I eventually shifted my focus to other areas, I hereby would like to share my insights and possibly facilitate future research. I hope this series of blog posts coule provide a comprehensive overview of RL4OR for those who come after me.</p><h2 id=what-is-operating-research-or>What is Operating Research (OR)</h2><p>At the beginning of my survey, I tried to search and understand what exactly Operating Research is:</p><blockquote><p>Operations Research (OR) is a branch of mathematics that deals with the application of scientific methods, mathematical models, and algorithms to decision making problems in real-world operations.</p></blockquote><p>According to <a href=https://en.wikipedia.org/wiki/Operations_research>Wikipedia</a>, the major sub-disciplines in modern operational research are:</p><pre><code class=language-text>|- Computing and information technologies
|- Financial engineering
|- Manufacturing, service sciences, and supply chain management
|- Policy modeling and public sector work
|- Revenue management
|- Simulation
|- Stochastic models
|- Transportation theory (mathematics)
|- Game theory for strategies
|- Linear programming
|- Nonlinear programming
|- Integer programming in NP-complete problem specially for 0-1 integer(linear programming for binary)
|- Dynamic programming in Aerospace engineering and Economics
|- Information theory used in Cryptography, Quantum computing
|- Quadratic programming for solutions of Quadratic equation and Quadratic function
</code></pre><h2 id=combinatorial-optimization-co>Combinatorial Optimization (CO)</h2><p>For some of us, it might be more familiar with another famous sub-concept in OR, <strong>Combinatorial Optimization (CO)</strong>, a subfield of mathematical optimization that consists of finding an optimal object from a finite set of objects, where the set of feasible solutions is discrete or can be reduced to a discrete set.</p><p>Specific problems:</p><pre><code class=language-text>- Assignment Problems:
    - Assignment problem
    - Generalized assignment problem
    - Quadratic assignment problem
    - Weapon target assignment problem
    - Resource allocation problems
- Closure problem
- Constraint satisfaction problem
- Cutting stock problem: Cutting small items out of bigger ones.
- Dominating set problem
- Integer programming
- Bin packing problem
    - Knapsack problem
    - Floorplanning: designing the layout of equipment in a factory or components on a computer chip to reduce manufacturing time (therefore reducing cost)
- Minimum relevant variables in linear system
- Minimum spanning tree
- Set cover problem: for instance, facility location problem or a network optimization problem (such as setup of telecommunications or power system networks to maintain quality of service during outages)
- Scheduling:
    - Personnel staffing
    - Manufacturing steps
    - Project tasks
    - Network data traffic: these are known as queueing models or queueing systems.
    - Sports events and their television coverage
    - Job shop scheduling
- Routing
    - Traveling salesman problem
    - Vehicle rescheduling problem
    - Vehicle routing problem
</code></pre><p>Because of my limited time of survey, I concluded three canonical Combinatorial Optimization problems on which researchers focus mostly: Mixed-Integer Program (MIP), Traveling Salesman Problem(TSP) and Bin Packing Problem (BPP).</p><h3 id=mixed-integer-program-mip>Mixed-Integer Program (MIP)</h3><p><b style=color:#68a0b4>Definition</b> <strong>Mixed-Integer Program (MIP)</strong></p><div>\[\begin{equation}
\begin{split}
\text{minimize} &\quad c^T x \\
\text{subject to} &\quad Ax \leq b \\
&\quad x_i \in \mathbb{Z} \quad \text{for} \quad i \in I \\
&\quad x_j \in \mathbb{R} \quad \text{for} \quad j \in J \\
\end{split}
\end{equation}\]
where $c$ is the coefficient vector, $x$ is the decision variable vector, $A$ is the constraint matrix, $b$ is the constraint vector, $I$ is the set of indices for integer variables, and $J$ is the set of indices for continuous variables.</div><p><img src=../mip.png alt="Graph Representation of MIPs" title="MIP Graph Representation"></p><p><cite>Rob Pike<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite></p><h3 id=traveling-salesman-problemtsp>Traveling Salesman Problem(TSP)</h3><p><b style=color:#68a0b4>Definition</b> <strong>Traveling Salesman Problem(TSP)</strong></p><p>Given a complete weighted graph $𝐺 = (𝑉, 𝐸)$, find a tour of minimum total weight, i.e. a cycle of minimum length that visits each node of the graph exactly once.</p><table><thead><tr><th style=text-align:center><img src=../chip.jpeg alt></th><th style=text-align:center><img src=../Power-Grid-Architecture.png alt></th><th style=text-align:center><img src=../Food-Delivery.jpeg alt></th></tr></thead><tbody><tr><td style=text-align:center>Chip Design</td><td style=text-align:center>Power Grid Design</td><td style=text-align:center>Food Delivery Route Planning</td></tr></tbody></table><p><img src=../tsp.png alt></p><h3 id=bin-packing-problem-bpp>Bin Packing Problem (BPP)</h3><p><b style=color:#68a0b4>Definition</b> <strong>Bin Packing Problem (BPP)</strong></p><table><thead><tr><th style=text-align:left><img src=../Bin-Parking2d.png alt></th><th style=text-align:left><img src=../Bin-Parking3d.png alt></th></tr></thead><tbody><tr><td style=text-align:left>2D Bin Packing</td><td style=text-align:left>3D Bin Packing</td></tr></tbody></table><table><thead><tr><th style=text-align:center><img src=../Packing-Plan.png alt></th><th style=text-align:center><img src=../Material-Cut-Plan.png alt></th><th style=text-align:center><img src=../Server-Farm.png alt></th></tr></thead><tbody><tr><td style=text-align:center>Packing Plan</td><td style=text-align:center>Material Cut Plan</td><td style=text-align:center>Task Assignment in Server Farm</td></tr></tbody></table><h2 id=how-mlrl-is-used-in-co>How ML/RL is used in CO</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>The above quote is excerpted from Rob Pike&rsquo;s <a href="https://www.youtube.com/watch?v=PAAkCSZUG1c">talk</a> during Gopherfest, November 18, 2015.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></article><div class=my-4><a href=https://linxiagong.github.io/tags/machine-learning/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#Machine Learning</a>
<a href=https://linxiagong.github.io/tags/reinforcement-learning/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#Reinforcement Learning</a>
<a href=https://linxiagong.github.io/tags/operation-research/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#Operation Research</a>
<a href=https://linxiagong.github.io/tags/combinatorial-optimization/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#Combinatorial Optimization</a></div></div><div class=col-span-2><div class="bg-secondary-bg prose max-w-none rounded p-6"><h3>Series of Posts</h3><a href=https://linxiagong.github.io/posts/rl4or/rl4or-1/ class=no-underline>Reinforcement Learning for Operation Research I</a><br></div><div class="bg-primary-bg
prose sticky top-16 z-10 hidden px-6 py-4 lg:block"><h3>On This Page</h3></div><div class="sticky-toc hidden px-6 pb-6 lg:block"><nav id=TableOfContents><ul><li><a href=#what-is-operating-research-or>What is Operating Research (OR)</a></li><li><a href=#combinatorial-optimization-co>Combinatorial Optimization (CO)</a><ul><li><a href=#mixed-integer-program-mip>Mixed-Integer Program (MIP)</a></li><li><a href=#traveling-salesman-problemtsp>Traveling Salesman Problem(TSP)</a></li><li><a href=#bin-packing-problem-bpp>Bin Packing Problem (BPP)</a></li></ul></li><li><a href=#how-mlrl-is-used-in-co>How ML/RL is used in CO</a></li></ul></nav></div><script>window.addEventListener("DOMContentLoaded",()=>{enableStickyToc()})</script></div></div><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll()})</script></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2021 <a href=https://www.wangchucheng.com/>WANG Chucheng</a> and <a href=https://www.ruiqima.com/>MA Ruiqi</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>